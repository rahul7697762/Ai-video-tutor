# YouTube Learning Assistant

An AI-powered YouTube tutor that provides contextual explanations when you don't understand something in a video.

## ğŸ¯ What It Does

1. **Extracts** the complete transcript from any YouTube video
2. **Indexes** the transcript into a RAG (Retrieval-Augmented Generation) system
3. **Explains** concepts when you pause and say "I didn't understand this"
4. **Speaks** the explanation with natural TTS (optional)

## ğŸ—ï¸ Architecture

See [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md) for the complete system design.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Browser Extensionâ”‚â”€â”€â”€â”€â–¶â”‚  Next.js Frontendâ”‚â”€â”€â”€â”€â–¶â”‚  Python Backend  â”‚
â”‚ (Pause Detection)â”‚     â”‚  (UI + Display)  â”‚     â”‚  (RAG + LLM + TTS)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
youtube-tutor/
â”œâ”€â”€ backend/                    # Python FastAPI backend
â”‚   â”œâ”€â”€ api/                    # REST API routes
â”‚   â”‚   â””â”€â”€ routes/
â”‚   â”œâ”€â”€ services/               # Business logic
â”‚   â”‚   â”œâ”€â”€ transcript/         # YouTube transcript extraction
â”‚   â”‚   â”œâ”€â”€ rag/                # Embedding + retrieval
â”‚   â”‚   â”œâ”€â”€ llm/                # LLM prompting
â”‚   â”‚   â””â”€â”€ tts/                # Text-to-speech
â”‚   â”œâ”€â”€ models/                 # Pydantic schemas
â”‚   â””â”€â”€ main.py                 # FastAPI entry
â”œâ”€â”€ frontend/                   # Vite React web app
â”‚   â”œâ”€â”€ src/                    # React components
â”‚   â””â”€â”€ public/                 # Static assets
â”œâ”€â”€ extension/                  # Browser extension
â”‚   â”œâ”€â”€ manifest.json
â”‚   â”œâ”€â”€ content/                # YouTube page scripts
â”‚   â””â”€â”€ popup/                  # Extension popup UI
â”œâ”€â”€ docs/                       # Documentation
â”‚   â””â”€â”€ ARCHITECTURE.md         # System design
â””â”€â”€ data/                       # Local data storage
    â”œâ”€â”€ chroma/                 # Vector database
    â””â”€â”€ audio/                  # TTS cache
```

---

## ğŸš€ Deployment

### Option 1: Deploy Backend to Render (Recommended - Free Tier)

1. **Push to GitHub** (if not already):
   ```bash
   git add .
   git commit -m "Prepare for deployment"
   git push origin main
   ```

2. **Deploy on Render**:
   - Go to [render.com](https://render.com)
   - Click "New +" â†’ "Web Service"
   - Connect your GitHub repository
   - **Root Directory**: `backend`
   - **Build Command**: `pip install -r requirements.txt`
   - **Start Command**: `uvicorn main:app --host 0.0.0.0 --port $PORT`
   - Add Environment Variables:
     - `OPENAI_API_KEY`: Your OpenAI API key
     - `FRONTEND_URL`: Your frontend deployment URL (after deploying frontend)
     - `LLM_PROVIDER`: `openai`
     - `TTS_PROVIDER`: `edge`

3. **Note your backend URL** (e.g., `https://ai-tutor-backend.onrender.com`)

### Option 2: Deploy Backend with Docker

```bash
cd backend
docker build -t ai-tutor-backend .
docker run -p 8000:8000 --env-file .env ai-tutor-backend
```

### Deploy Frontend to Vercel (Recommended)

1. **Deploy on Vercel**:
   - Go to [vercel.com](https://vercel.com)
   - Import your GitHub repository
   - **Root Directory**: `frontend`
   - **Framework Preset**: Vite
   - Add Environment Variable:
     - `VITE_API_URL`: Your backend URL (e.g., `https://ai-tutor-backend.onrender.com`)

2. **Update Backend CORS** (on Render):
   - Add `FRONTEND_URL` environment variable with your Vercel URL

### Deploy Frontend to Netlify (Alternative)

1. **Deploy on Netlify**:
   - Go to [netlify.com](https://netlify.com)
   - Connect your GitHub repository
   - **Base Directory**: `frontend`
   - **Build Command**: `npm run build`
   - **Publish Directory**: `dist`
   - Add Environment Variable:
     - `VITE_API_URL`: Your backend URL

---

## ğŸ› ï¸ Local Development

### Prerequisites

- Python 3.11+
- Node.js 18+
- OpenAI API key (for embeddings + LLM)
- ElevenLabs API key (optional, for TTS)

### Backend Setup

```bash
cd backend
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Mac/Linux

pip install -r requirements.txt

# Set environment variables
cp .env.example .env
# Edit .env with your API keys

# Run development server
uvicorn main:app --reload --port 8000
```

### Frontend Setup

```bash
cd frontend
npm install
npm run dev
```

### Extension Setup

1. Open Chrome/Edge â†’ Extensions â†’ Developer Mode
2. Click "Load unpacked"
3. Select the `extension/` folder

---

## ğŸ”§ Configuration

### Backend Environment Variables

| Variable | Description | Required |
|----------|-------------|----------|
| `OPENAI_API_KEY` | OpenAI API key for embeddings and LLM | Yes |
| `ELEVENLABS_API_KEY` | ElevenLabs API key for TTS | No |
| `ANTHROPIC_API_KEY` | Anthropic API key (if using Claude) | No |
| `FRONTEND_URL` | Production frontend URL for CORS | For Prod |
| `CHROMA_PERSIST_DIR` | ChromaDB storage path | No (default: `./data/chroma`) |
| `LLM_PROVIDER` | LLM provider: `openai` or `anthropic` | No (default: `openai`) |
| `LLM_MODEL` | LLM model name | No (default: `gpt-4o-mini`) |
| `TTS_PROVIDER` | TTS provider: `elevenlabs`, `openai`, or `edge` | No (default: `edge`) |

### Frontend Environment Variables

| Variable | Description | Required |
|----------|-------------|----------|
| `VITE_API_URL` | Backend API URL | For Prod |

---

## ğŸ“– API Endpoints

### GET `/api/health`
Health check endpoint.

### POST `/api/transcript/ingest`
Ingest a YouTube video transcript into the RAG system.

```json
{
  "video_id": "dQw4w9WgXcQ"
}
```

### POST `/api/explain`
Get an explanation for a specific timestamp.

```json
{
  "video_id": "dQw4w9WgXcQ",
  "timestamp": 125.5,
  "user_query": "I don't understand this",
  "include_audio": true
}
```

### POST `/api/explain/stream`
Same as above but streams the response via SSE.

---

## ğŸ§  How RAG Works

1. **Chunking**: Transcript is split into 20-40 second semantic chunks with 5s overlap
2. **Embedding**: Each chunk is embedded using `text-embedding-3-small`
3. **Retrieval** (3 strategies):
   - **Temporal**: Chunks within Â±60s of pause timestamp
   - **Foundational**: Earlier chunks that define terms used
   - **Semantic**: Similar chunks from anywhere in video
4. **Generation**: Retrieved chunks are sent to LLM with tutor prompt

---

## ğŸ¤ TTS Options

| Provider | Quality | Cost | Speed |
|----------|---------|------|-------|
| ElevenLabs Turbo | High | $0.15/1K chars | Fast |
| OpenAI TTS | Good | $0.015/1K chars | Fast |
| Edge TTS | Medium | Free | Medium |

---

## ğŸ“Š Cost Estimates

| Operation | Estimated Cost |
|-----------|---------------|
| Ingest 1 video (10 min) | ~$0.001 |
| 1 explanation (text only) | ~$0.01 |
| 1 explanation (text + audio) | ~$0.03 |

---

## ğŸ› ï¸ Development Commands

### Run Tests
```bash
cd backend
pytest tests/
```

### Type Checking
```bash
cd backend
mypy .
```

### Lint
```bash
cd backend
ruff check .
```

### Build Frontend
```bash
cd frontend
npm run build
```

---

## ğŸ“„ License

MIT
